name: Evaluate Denoising Autoencoder Model V1.8
inputs:
  - {name: trained_model, type: Data}
  - {name: test_dataset, type: Data}
  - {name: batch_size, type: Integer, optional: true, default: "32"}
  - {name: num_workers, type: Integer, optional: true, default: "0"}
  - {name: noise_type, type: String, optional: true, default: "gaussian"}
  - {name: noise_factor, type: Float, optional: true, default: "0.3"}
  - {name: compute_metrics, type: String, optional: true, default: "True"}
  - {name: seed, type: Integer, optional: true, default: "42"}

outputs:
  - {name: evaluation_results, type: Data}
  - {name: reconstruction_samples, type: Data}
  - {name: schema_json, type: String}

implementation:
  container:
    image: kushagra4761/nesy-factory-gpu:t2.6v4
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import logging
        import random
        import time
        import numpy as np
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, TensorDataset
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("model_evaluator")
        
        parser = argparse.ArgumentParser()
        parser.add_argument('--trained_model', required=True)
        parser.add_argument('--test_dataset', required=True)
        parser.add_argument('--batch_size', type=int, default=32)
        parser.add_argument('--num_workers', type=int, default=0)
        parser.add_argument('--noise_type', type=str, default='gaussian')
        parser.add_argument('--noise_factor', type=float, default=0.3)
        parser.add_argument('--compute_metrics', type=lambda x: x.lower() == 'true', default=True)
        parser.add_argument('--seed', type=int, default=42)
        parser.add_argument('--evaluation_results', required=True)
        parser.add_argument('--reconstruction_samples', required=True)
        parser.add_argument('--schema_json', required=True)
        args = parser.parse_args()
        
        def set_seed(seed):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            logger.info(f"✓ Random seed set to: {seed}")
        
        def get_activation(activation_name):
            activations = {
                'relu': nn.ReLU(),
                'leaky_relu': nn.LeakyReLU(0.2),
                'elu': nn.ELU(),
                'selu': nn.SELU(),
                'gelu': nn.GELU(),
                'tanh': nn.Tanh(),
                'sigmoid': nn.Sigmoid(),
                'swish': nn.SiLU(),
                'mish': nn.Mish(),
                'prelu': nn.PReLU(),
            }
            return activations.get(activation_name.lower(), nn.ReLU())
        
        class DenoisingAutoencoder(nn.Module):
            def __init__(self, input_dim=128, hidden_dims=[96, 64], latent_dim=64, dropout=0.2, activation='relu', use_batch_norm=True):
                super(DenoisingAutoencoder, self).__init__()
                self.input_dim = input_dim
                self.latent_dim = latent_dim
                self.use_batch_norm = use_batch_norm
                encoder_layers = []
                encoder_layers.append(nn.Linear(input_dim, hidden_dims[0]))
                if use_batch_norm:
                    encoder_layers.append(nn.BatchNorm1d(hidden_dims[0]))
                encoder_layers.append(get_activation(activation))
                encoder_layers.append(nn.Dropout(dropout))
                encoder_layers.append(nn.Linear(hidden_dims[0], hidden_dims[1]))
                if use_batch_norm:
                    encoder_layers.append(nn.BatchNorm1d(hidden_dims[1]))
                encoder_layers.append(get_activation(activation))
                encoder_layers.append(nn.Dropout(dropout))
                encoder_layers.append(nn.Linear(hidden_dims[1], latent_dim))
                self.encoder = nn.Sequential(*encoder_layers)
                decoder_layers = []
                decoder_layers.append(nn.Linear(latent_dim, hidden_dims[1]))
                if use_batch_norm:
                    decoder_layers.append(nn.BatchNorm1d(hidden_dims[1]))
                decoder_layers.append(get_activation(activation))
                decoder_layers.append(nn.Dropout(dropout))
                decoder_layers.append(nn.Linear(hidden_dims[1], hidden_dims[0]))
                if use_batch_norm:
                    decoder_layers.append(nn.BatchNorm1d(hidden_dims[0]))
                decoder_layers.append(get_activation(activation))
                decoder_layers.append(nn.Dropout(dropout))
                decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
                self.decoder = nn.Sequential(*decoder_layers)
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded, encoded
            
            def encode(self, x):
                return self.encoder(x)
            
            def decode(self, z):
                return self.decoder(z)
        
        def add_noise_to_latent(latent_vectors, noise_type='gaussian', noise_factor=0.3):
            if noise_type == 'gaussian':
                noise = torch.randn_like(latent_vectors) * noise_factor
                noisy = latent_vectors + noise
            elif noise_type == 'salt_pepper':
                noisy = latent_vectors.clone()
                mask = torch.rand_like(latent_vectors) < noise_factor
                random_vals = torch.rand(mask.sum(), device=latent_vectors.device)
                random_vals = random_vals * (latent_vectors.max() - latent_vectors.min()) + latent_vectors.min()
                noisy[mask] = random_vals
            elif noise_type == 'dropout':
                mask = torch.rand_like(latent_vectors) > noise_factor
                noisy = latent_vectors * mask
            elif noise_type == 'uniform':
                noise = (torch.rand_like(latent_vectors) - 0.5) * 2 * noise_factor
                noisy = latent_vectors + noise
            else:
                raise ValueError(f"Unknown noise type: {noise_type}")
            return noisy
        
        def compute_reconstruction_metrics(original, reconstructed):
            original_np = original.cpu().numpy()
            reconstructed_np = reconstructed.cpu().numpy()
            metrics = {}
            metrics['mse'] = mean_squared_error(original_np.flatten(), reconstructed_np.flatten())
            metrics['rmse'] = np.sqrt(metrics['mse'])
            metrics['mae'] = mean_absolute_error(original_np.flatten(), reconstructed_np.flatten())
            metrics['r2_score'] = r2_score(original_np.flatten(), reconstructed_np.flatten())
            per_sample_mse = np.mean((original_np - reconstructed_np) ** 2, axis=1)
            metrics['mean_per_sample_mse'] = float(np.mean(per_sample_mse))
            metrics['std_per_sample_mse'] = float(np.std(per_sample_mse))
            metrics['min_per_sample_mse'] = float(np.min(per_sample_mse))
            metrics['max_per_sample_mse'] = float(np.max(per_sample_mse))
            from sklearn.metrics.pairwise import cosine_similarity
            cos_sim = []
            for i in range(len(original_np)):
                sim = cosine_similarity(original_np[i:i+1], reconstructed_np[i:i+1])[0, 0]
                cos_sim.append(sim)
            metrics['mean_cosine_similarity'] = float(np.mean(cos_sim))
            metrics['std_cosine_similarity'] = float(np.std(cos_sim))
            signal_power = np.mean(original_np ** 2)
            noise_power = np.mean((original_np - reconstructed_np) ** 2)
            if noise_power > 0:
                metrics['snr_db'] = float(10 * np.log10(signal_power / noise_power))
            else:
                metrics['snr_db'] = float('inf')
            return metrics
        
        def evaluate_model(model, dataloader, device, noise_type='gaussian', noise_factor=0.3):
            model.eval()
            all_originals = []
            all_noisy = []
            all_reconstructed = []
            all_encoded = []
            with torch.no_grad():
                for batch_idx, (clean_latents,) in enumerate(dataloader):
                    clean_latents = clean_latents.to(device)
                    noisy_latents = add_noise_to_latent(clean_latents, noise_type, noise_factor)
                    reconstructed, encoded = model(noisy_latents)
                    all_originals.append(clean_latents.cpu())
                    all_noisy.append(noisy_latents.cpu())
                    all_reconstructed.append(reconstructed.cpu())
                    all_encoded.append(encoded.cpu())
            all_originals = torch.cat(all_originals, dim=0)
            all_noisy = torch.cat(all_noisy, dim=0)
            all_reconstructed = torch.cat(all_reconstructed, dim=0)
            all_encoded = torch.cat(all_encoded, dim=0)
            return all_originals, all_noisy, all_reconstructed, all_encoded
        
        try:
            set_seed(args.seed)
            if torch.cuda.is_available():
                device = 'cuda'
                logger.info(f"✓ GPU detected: {torch.cuda.get_device_name(0)}")
            else:
                device = 'cpu'
                logger.info("⚠ No GPU detected, using CPU")
            
            logger.info("="*60)
            logger.info("Loading Trained Model")
            logger.info("="*60)
            model_checkpoint_path = os.path.join(args.trained_model, 'best_trained_model.pth')
            if not os.path.exists(model_checkpoint_path):
                raise FileNotFoundError(f"Trained model not found at: {model_checkpoint_path}")
            checkpoint = torch.load(model_checkpoint_path, map_location=device)
            logger.info(f"✓ Loaded trained model from: {model_checkpoint_path}")
            logger.info(f"  Epoch: {checkpoint['epoch']}")
            logger.info(f"  Training loss: {checkpoint['train_loss']:.6f}")
            logger.info(f"  Validation loss: {checkpoint['val_loss']:.6f}")
            init_checkpoint_paths = [
                os.path.join(os.path.dirname(args.trained_model), 'initialized_model.pt'),
                os.path.join(args.trained_model, 'initialized_model.pt')
            ]
            init_checkpoint = None
            for path in init_checkpoint_paths:
                if os.path.exists(path):
                    init_checkpoint = torch.load(path, map_location=device)
                    logger.info(f"✓ Loaded initialization config from: {path}")
                    break
            if init_checkpoint is None:
                logger.warning("Could not find initialization checkpoint. Using default configuration.")
                model_config = {
                    'input_dim': 128,
                    'hidden_dims': [96, 64],
                    'latent_dim': 64,
                    'dropout': 0.2,
                    'activation_function': 'relu',
                    'use_batch_norm': True
                }
            else:
                model_config = init_checkpoint['model_config']
            logger.info(f"Model configuration: {model_config}")
            logger.info("="*60)
            logger.info("Reconstructing Model")
            logger.info("="*60)
            model = DenoisingAutoencoder(
                input_dim=model_config['input_dim'],
                hidden_dims=model_config['hidden_dims'],
                latent_dim=model_config['latent_dim'],
                dropout=model_config['dropout'],
                activation=model_config.get('activation_function', 'relu'),
                use_batch_norm=model_config.get('use_batch_norm', True)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(device)
            model.eval()
            logger.info(f"✓ Model reconstructed and loaded to {device}")
            logger.info(f"  Total parameters: {sum(p.numel() for p in model.parameters()):,}")
            logger.info("="*60)
            logger.info("Loading Test Dataset")
            logger.info("="*60)
            test_dataset_path = os.path.join(args.test_dataset, 'test_dataset.pt')
            test_data = torch.load(test_dataset_path, map_location='cpu')
            test_dataset = TensorDataset(test_data)
            logger.info(f"✓ Loaded test dataset: {test_data.shape}")
            test_loader = DataLoader(
                test_dataset,
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=args.num_workers,
                pin_memory=True if device == 'cuda' else False
            )
            logger.info(f"✓ Test batches: {len(test_loader)}")
            logger.info(f"  Total test samples: {len(test_dataset)}")
            logger.info("="*60)
            logger.info("Evaluating Model on Test Set")
            logger.info("="*60)
            logger.info(f"Noise type: {args.noise_type}")
            logger.info(f"Noise factor: {args.noise_factor}")
            logger.info("="*60)
            all_originals, all_noisy, all_reconstructed, all_encoded = evaluate_model(
                model=model,
                dataloader=test_loader,
                device=device,
                noise_type=args.noise_type,
                noise_factor=args.noise_factor
            )
            logger.info(f"✓ Evaluation complete")
            logger.info(f"  Processed {len(all_originals)} samples")
            if args.compute_metrics:
                logger.info("="*60)
                logger.info("Computing Evaluation Metrics")
                logger.info("="*60)
                metrics = compute_reconstruction_metrics(all_originals, all_reconstructed)
                logger.info("Reconstruction Metrics:")
                logger.info(f"  MSE: {metrics['mse']:.6f}")
                logger.info(f"  RMSE: {metrics['rmse']:.6f}")
                logger.info(f"  MAE: {metrics['mae']:.6f}")
                logger.info(f"  R² Score: {metrics['r2_score']:.6f}")
                logger.info(f"  Mean Cosine Similarity: {metrics['mean_cosine_similarity']:.6f}")
                logger.info(f"  SNR (dB): {metrics['snr_db']:.2f}")
                logger.info(f"  Per-Sample MSE (mean ± std): {metrics['mean_per_sample_mse']:.6f} ± {metrics['std_per_sample_mse']:.6f}")
            else:
                metrics = {}
            
            logger.info("="*60)
            logger.info("Saving Schema JSON")
            logger.info("="*60)
            current_timestamp = int(time.time())
            schema_data = {
                'timestamp': current_timestamp,
                'r2_score': float(metrics.get('r2_score', 0.0) * 100),
                'mae': float(metrics.get('mae', 0.0) * 100),
                'mse': float(metrics.get('mse', 0.0) * 100),
                'rmse': float(metrics.get('rmse', 0.0) * 100)
            }
            schema_json_str = json.dumps(schema_data)
            with open(args.schema_json, 'w') as f:
                f.write(schema_json_str)
            logger.info(f"✓ Schema JSON saved to: {args.schema_json}")
            logger.info(f"  Timestamp: {current_timestamp}")
            logger.info(f"  R² Score (×100): {schema_data['r2_score']:.2f}")
            logger.info(f"  MAE (×100): {schema_data['mae']:.2f}")
            logger.info(f"  MSE (×100): {schema_data['mse']:.2f}")
            logger.info(f"  RMSE (×100): {schema_data['rmse']:.2f}")
            
            logger.info("="*60)
            logger.info("Saving Evaluation Results")
            logger.info("="*60)
            os.makedirs(args.evaluation_results, exist_ok=True)
            evaluation_results = {
                'metrics': metrics,
                'test_samples': len(all_originals),
                'evaluation_config': {
                    'noise_type': args.noise_type,
                    'noise_factor': args.noise_factor,
                    'batch_size': args.batch_size
                },
                'model_info': {
                    'training_epoch': int(checkpoint['epoch']),
                    'training_loss': float(checkpoint['train_loss']),
                    'validation_loss': float(checkpoint['val_loss'])
                }
            }
            results_path = os.path.join(args.evaluation_results, 'evaluation_results.json')
            with open(results_path, 'w') as f:
                json.dump(evaluation_results, f, indent=2)
            logger.info(f"✓ Evaluation results saved to: {results_path}")
            with open(args.evaluation_results + ".meta.json", "w") as f:
                json.dump({
                    "results_path": results_path,
                    "test_samples": len(all_originals),
                    "mse": metrics.get('mse', 0.0)
                }, f, indent=2)
            
            logger.info("="*60)
            logger.info("Saving Reconstruction Samples")
            logger.info("="*60)
            os.makedirs(args.reconstruction_samples, exist_ok=True)
            num_samples_to_save = min(100, len(all_originals))
            samples_data = {
                'original': all_originals[:num_samples_to_save],
                'noisy': all_noisy[:num_samples_to_save],
                'reconstructed': all_reconstructed[:num_samples_to_save],
                'encoded': all_encoded[:num_samples_to_save]
            }
            samples_path = os.path.join(args.reconstruction_samples, 'reconstruction_samples.pt')
            torch.save(samples_data, samples_path)
            logger.info(f"✓ Saved {num_samples_to_save} reconstruction samples to: {samples_path}")
            with open(args.reconstruction_samples + ".meta.json", "w") as f:
                json.dump({
                    "samples_path": samples_path,
                    "num_samples": num_samples_to_save
                }, f, indent=2)
            
            logger.info("="*60)
            logger.info("✓ Evaluation Complete!")
            logger.info("="*60)
            logger.info(f"Results: {results_path}")
            logger.info(f"Schema JSON: {args.schema_json}")
            logger.info(f"Test MSE: {metrics.get('mse', 0.0):.6f}")
            logger.info(f"Test RMSE: {metrics.get('rmse', 0.0):.6f}")
            logger.info(f"R² Score: {metrics.get('r2_score', 0.0):.6f}")
            
        except Exception as e:
            logger.exception(f"Fatal error during evaluation: {str(e)}")
            sys.exit(1)

    args:
      - --trained_model
      - {inputPath: trained_model}
      - --test_dataset
      - {inputPath: test_dataset}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --noise_type
      - {inputValue: noise_type}
      - --noise_factor
      - {inputValue: noise_factor}
      - --compute_metrics
      - {inputValue: compute_metrics}
      - --seed
      - {inputValue: seed}
      - --evaluation_results
      - {outputPath: evaluation_results}
      - --reconstruction_samples
      - {outputPath: reconstruction_samples}
      - --schema_json
      - {outputPath: schema_json}
