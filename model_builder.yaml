name: Initialize Denoising Autoencoder Model
inputs:
  - {name: input_dim, type: Integer, optional: true, default: "128"}
  - {name: hidden_dim_1, type: Integer, optional: true, default: "96"}
  - {name: hidden_dim_2, type: Integer, optional: true, default: "64"}
  - {name: latent_dim, type: Integer, optional: true, default: "64"}
  - {name: dropout, type: Float, optional: true, default: "0.2"}
  - {name: weight_init_method, type: String, optional: true, default: "xavier_uniform"}
  - {name: activation_function, type: String, optional: true, default: "relu"}
  - {name: use_batch_norm, type: String, optional: true, default: "true"}
  - {name: optimizer_type, type: String, optional: true, default: "adam"}
  - {name: learning_rate, type: Float, optional: true, default: "0.001"}
  - {name: weight_decay, type: Float, optional: true, default: "0.00001"}
  - {name: loss_type, type: String, optional: true, default: "mse"}
  - {name: seed, type: Integer, optional: true, default: "42"}

outputs:
  - {name: model_output, type: Data}
  - {name: model_config, type: Data}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import logging
        import random
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        # Setup logging
        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("model_initializer")
        
        # Parse arguments
        parser = argparse.ArgumentParser()
        parser.add_argument('--input_dim', type=int, default=128)
        parser.add_argument('--hidden_dim_1', type=int, default=96)
        parser.add_argument('--hidden_dim_2', type=int, default=64)
        parser.add_argument('--latent_dim', type=int, default=64)
        parser.add_argument('--dropout', type=float, default=0.2)
        parser.add_argument('--weight_init_method', type=str, default='xavier_uniform')
        parser.add_argument('--activation_function', type=str, default='relu')
        parser.add_argument('--use_batch_norm', type=lambda x: x.lower() == 'true', default="true")
        parser.add_argument('--optimizer_type', type=str, default='adam')
        parser.add_argument('--learning_rate', type=float, default=1e-3)
        parser.add_argument('--weight_decay', type=float, default=1e-5)
        parser.add_argument('--loss_type', type=str, default='mse')
        parser.add_argument('--seed', type=int, default=42)
        parser.add_argument('--model_output', required=True)
        parser.add_argument('--model_config', required=True)
        args = parser.parse_args()
        
        # ========================================
        # Set Random Seeds for Reproducibility
        # ========================================
        def set_seed(seed):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            logger.info(f"✓ Random seed set to: {seed}")
        
        # ========================================
        # Weight Initialization Functions
        # ========================================
        def initialize_weights(model, method='xavier_uniform', activation='relu'):
            logger.info(f"Initializing weights with method: {method.upper()}")
            
            for name, module in model.named_modules():
                if isinstance(module, nn.Linear):
                    if method == 'xavier_uniform':
                        nn.init.xavier_uniform_(module.weight)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'xavier_normal':
                        nn.init.xavier_normal_(module.weight)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'kaiming_uniform':
                        # He initialization for ReLU
                        nonlinearity = 'relu' if activation.lower() == 'relu' else 'leaky_relu'
                        nn.init.kaiming_uniform_(module.weight, mode='fan_in', nonlinearity=nonlinearity)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'kaiming_normal':
                        # He initialization for ReLU
                        nonlinearity = 'relu' if activation.lower() == 'relu' else 'leaky_relu'
                        nn.init.kaiming_normal_(module.weight, mode='fan_in', nonlinearity=nonlinearity)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'orthogonal':
                        nn.init.orthogonal_(module.weight, gain=1.0)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'normal':
                        nn.init.normal_(module.weight, mean=0.0, std=0.02)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'uniform':
                        nn.init.uniform_(module.weight, a=-0.1, b=0.1)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'sparse':
                        nn.init.sparse_(module.weight, sparsity=0.1)
                        if module.bias is not None:
                            nn.init.zeros_(module.bias)
                    
                    elif method == 'default':
                        # PyTorch default initialization (do nothing)
                        pass
                    
                    else:
                        logger.warning(f"Unknown initialization method: {method}. Using default.")
                
                elif isinstance(module, nn.BatchNorm1d):
                    # BatchNorm initialization
                    if module.weight is not None:
                        nn.init.ones_(module.weight)
                    if module.bias is not None:
                        nn.init.zeros_(module.bias)
            
            logger.info("✓ Weight initialization complete")
        
        # ========================================
        # Get Activation Function
        # ========================================
        def get_activation(activation_name):
            activations = {
                'relu': nn.ReLU(),
                'leaky_relu': nn.LeakyReLU(0.2),
                'elu': nn.ELU(),
                'selu': nn.SELU(),
                'gelu': nn.GELU(),
                'tanh': nn.Tanh(),
                'sigmoid': nn.Sigmoid(),
                'swish': nn.SiLU(),  # Swish is SiLU in PyTorch
                'mish': nn.Mish(),
                'prelu': nn.PReLU(),
            }
            
            if activation_name.lower() not in activations:
                logger.warning(f"Unknown activation: {activation_name}. Using ReLU.")
                return nn.ReLU()
            
            return activations[activation_name.lower()]
        
        # ========================================
        # Model Definition
        # ========================================
        class DenoisingAutoencoder(nn.Module):
            def __init__(self, input_dim=128, hidden_dims=[96, 64], latent_dim=64, 
                         dropout=0.2, activation='relu', use_batch_norm=True):
                super(DenoisingAutoencoder, self).__init__()
                
                self.input_dim = input_dim
                self.latent_dim = latent_dim
                self.use_batch_norm = use_batch_norm
                
                # Get activation function
                act_fn = get_activation(activation)
                
                # Encoder layers
                encoder_layers = []
                
                # First encoder layer
                encoder_layers.append(nn.Linear(input_dim, hidden_dims[0]))
                if use_batch_norm:
                    encoder_layers.append(nn.BatchNorm1d(hidden_dims[0]))
                encoder_layers.append(get_activation(activation))
                encoder_layers.append(nn.Dropout(dropout))
                
                # Second encoder layer
                encoder_layers.append(nn.Linear(hidden_dims[0], hidden_dims[1]))
                if use_batch_norm:
                    encoder_layers.append(nn.BatchNorm1d(hidden_dims[1]))
                encoder_layers.append(get_activation(activation))
                encoder_layers.append(nn.Dropout(dropout))
                
                # Latent layer
                encoder_layers.append(nn.Linear(hidden_dims[1], latent_dim))
                
                self.encoder = nn.Sequential(*encoder_layers)
                
                # Decoder layers (mirror of encoder)
                decoder_layers = []
                
                # First decoder layer
                decoder_layers.append(nn.Linear(latent_dim, hidden_dims[1]))
                if use_batch_norm:
                    decoder_layers.append(nn.BatchNorm1d(hidden_dims[1]))
                decoder_layers.append(get_activation(activation))
                decoder_layers.append(nn.Dropout(dropout))
                
                # Second decoder layer
                decoder_layers.append(nn.Linear(hidden_dims[1], hidden_dims[0]))
                if use_batch_norm:
                    decoder_layers.append(nn.BatchNorm1d(hidden_dims[0]))
                decoder_layers.append(get_activation(activation))
                decoder_layers.append(nn.Dropout(dropout))
                
                # Output layer
                decoder_layers.append(nn.Linear(hidden_dims[0], input_dim))
                
                self.decoder = nn.Sequential(*decoder_layers)
            
            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded, encoded
            
            def encode(self, x):
                return self.encoder(x)
            
            def decode(self, z):
                return self.decoder(z)
        
        # ========================================
        # Setup Functions
        # ========================================
        def setup_optimizer_and_loss(model, optimizer_type='adam', lr=1e-3, 
                                    weight_decay=1e-5, loss_type='mse'):
            
            # ---- Optimizer Selection ----
            if optimizer_type.lower() == 'adam':
                optimizer = optim.Adam(
                    model.parameters(),
                    lr=lr,
                    betas=(0.9, 0.999),
                    eps=1e-8,
                    weight_decay=weight_decay
                )
            
            elif optimizer_type.lower() == 'adamw':
                optimizer = optim.AdamW(
                    model.parameters(),
                    lr=lr,
                    betas=(0.9, 0.999),
                    eps=1e-8,
                    weight_decay=weight_decay
                )
            
            elif optimizer_type.lower() == 'sgd':
                optimizer = optim.SGD(
                    model.parameters(),
                    lr=lr,
                    momentum=0.9,
                    weight_decay=weight_decay,
                    nesterov=True
                )
            
            elif optimizer_type.lower() == 'rmsprop':
                optimizer = optim.RMSprop(
                    model.parameters(),
                    lr=lr,
                    alpha=0.99,
                    eps=1e-8,
                    weight_decay=weight_decay,
                    momentum=0.9
                )
            
            elif optimizer_type.lower() == 'adagrad':
                optimizer = optim.Adagrad(
                    model.parameters(),
                    lr=lr,
                    lr_decay=0,
                    weight_decay=weight_decay
                )
            
            elif optimizer_type.lower() == 'adadelta':
                optimizer = optim.Adadelta(
                    model.parameters(),
                    lr=lr,
                    rho=0.9,
                    eps=1e-6,
                    weight_decay=weight_decay
                )
            
            else:
                raise ValueError(f"Unknown optimizer: {optimizer_type}")
            
            # ---- Loss Function Selection ----
            if loss_type.lower() == 'mse':
                criterion = nn.MSELoss()
            
            elif loss_type.lower() == 'l1':
                criterion = nn.L1Loss()
            
            elif loss_type.lower() == 'smooth_l1':
                criterion = nn.SmoothL1Loss()
            
            elif loss_type.lower() == 'huber':
                criterion = nn.HuberLoss(delta=1.0)
            
            elif loss_type.lower() == 'cosine':
                criterion = nn.CosineEmbeddingLoss()
            
            else:
                raise ValueError(f"Unknown loss type: {loss_type}")
            
            logger.info(f"Optimizer: {optimizer_type.upper()}")
            logger.info(f"Learning rate: {lr}")
            logger.info(f"Weight decay: {weight_decay}")
            logger.info(f"Loss function: {loss_type.upper()}")
            
            return optimizer, criterion
        
        def initialize_model(input_dim=128, hidden_dims=[96, 64], latent_dim=64, 
                             dropout=0.2, activation='relu', use_batch_norm=True, device='cuda'):
            model = DenoisingAutoencoder(
                input_dim=input_dim,
                hidden_dims=hidden_dims,
                latent_dim=latent_dim,
                dropout=dropout,
                activation=activation,
                use_batch_norm=use_batch_norm
            )
            
            model = model.to(device)
            
            # Print model architecture
            logger.info("Model Architecture:")
            logger.info(str(model))
            logger.info(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
            logger.info(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
            
            return model
        
        try:
            # ========================================
            # Set Random Seed
            # ========================================
            set_seed(args.seed)
            
            # ========================================
            # Device Detection
            # ========================================
            if torch.cuda.is_available():
                device = 'cuda'
                logger.info(f"✓ GPU detected: {torch.cuda.get_device_name(0)}")
                logger.info(f"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
                logger.info(f"  CUDA version: {torch.version.cuda}")
            else:
                device = 'cpu'
                logger.info("⚠ No GPU detected, using CPU")
            
            logger.info(f"Using device: {device}")
            
            # ========================================
            # Initialize Model
            # ========================================
            hidden_dims = [args.hidden_dim_1, args.hidden_dim_2]
            
            logger.info("="*50)
            logger.info("Initializing Denoising Autoencoder")
            logger.info("="*50)
            logger.info(f"Input dimension: {args.input_dim}")
            logger.info(f"Hidden dimensions: {hidden_dims}")
            logger.info(f"Latent dimension: {args.latent_dim}")
            logger.info(f"Dropout: {args.dropout}")
            logger.info(f"Activation function: {args.activation_function}")
            logger.info(f"Batch normalization: {args.use_batch_norm}")
            
            model = initialize_model(
                input_dim=args.input_dim,
                hidden_dims=hidden_dims,
                latent_dim=args.latent_dim,
                dropout=args.dropout,
                activation=args.activation_function,
                use_batch_norm=args.use_batch_norm,
                device=device
            )
            
            # ========================================
            # Apply Weight Initialization
            # ========================================
            logger.info("="*50)
            logger.info("Applying Weight Initialization")
            logger.info("="*50)
            
            initialize_weights(model, method=args.weight_init_method, activation=args.activation_function)
            
            # ========================================
            # Setup Optimizer and Loss
            # ========================================
            logger.info("="*50)
            logger.info("Setting up Optimizer and Loss Function")
            logger.info("="*50)
            
            optimizer, criterion = setup_optimizer_and_loss(
                model=model,
                optimizer_type=args.optimizer_type,
                lr=args.learning_rate,
                weight_decay=args.weight_decay,
                loss_type=args.loss_type
            )
            
            # ========================================
            # Save Model and Configuration
            # ========================================
            logger.info("="*50)
            logger.info("Saving Model and Configuration")
            logger.info("="*50)
            
            # Create output directories
            os.makedirs(args.model_output, exist_ok=True)
            os.makedirs(args.model_config, exist_ok=True)
            
            # Save model checkpoint
            model_path = os.path.join(args.model_output, 'initialized_model.pt')
            checkpoint = {
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'model_config': {
                    'input_dim': args.input_dim,
                    'hidden_dims': hidden_dims,
                    'latent_dim': args.latent_dim,
                    'dropout': args.dropout,
                    'activation_function': args.activation_function,
                    'use_batch_norm': args.use_batch_norm,
                    'weight_init_method': args.weight_init_method
                },
                'optimizer_config': {
                    'type': args.optimizer_type,
                    'lr': args.learning_rate,
                    'weight_decay': args.weight_decay
                },
                'loss_type': args.loss_type,
                'device': device,
                'seed': args.seed
            }
            
            torch.save(checkpoint, model_path)
            logger.info(f"✓ Model saved to: {model_path}")
            
            # Save model metadata
            with open(args.model_output + ".meta.json", "w") as f:
                json.dump({
                    "model_path": model_path,
                    "device": device,
                    "total_parameters": sum(p.numel() for p in model.parameters()),
                    "trainable_parameters": sum(p.numel() for p in model.parameters() if p.requires_grad)
                }, f, indent=2)
            
            # Save detailed configuration
            config_path = os.path.join(args.model_config, 'model_config.json')
            config = {
                'architecture': {
                    'input_dim': args.input_dim,
                    'hidden_dim_1': args.hidden_dim_1,
                    'hidden_dim_2': args.hidden_dim_2,
                    'latent_dim': args.latent_dim,
                    'dropout': args.dropout,
                    'activation_function': args.activation_function,
                    'use_batch_norm': args.use_batch_norm,
                    'weight_init_method': args.weight_init_method
                },
                'optimizer': {
                    'type': args.optimizer_type,
                    'learning_rate': args.learning_rate,
                    'weight_decay': args.weight_decay
                },
                'loss': {
                    'type': args.loss_type
                },
                'device': {
                    'type': device,
                    'cuda_available': torch.cuda.is_available(),
                    'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
                    'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
                },
                'model_statistics': {
                    'total_parameters': sum(p.numel() for p in model.parameters()),
                    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),
                    'encoder_parameters': sum(p.numel() for p in model.encoder.parameters()),
                    'decoder_parameters': sum(p.numel() for p in model.decoder.parameters())
                },
                'reproducibility': {
                    'seed': args.seed,
                    'pytorch_version': torch.__version__
                }
            }
            
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            logger.info(f"✓ Configuration saved to: {config_path}")
            
            # Save config metadata
            with open(args.model_config + ".meta.json", "w") as f:
                json.dump({
                    "config_path": config_path
                }, f, indent=2)
            
            logger.info("="*50)
            logger.info("✓ Model Initialization Complete!")
            logger.info("="*50)
            logger.info(f"Model checkpoint: {model_path}")
            logger.info(f"Configuration: {config_path}")
            logger.info(f"Device: {device}")
            logger.info(f"Weight initialization: {args.weight_init_method}")
            logger.info(f"Activation function: {args.activation_function}")
            logger.info(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
            logger.info(f"Random seed: {args.seed}")
            
        except Exception as e:
            logger.exception(f"Fatal error during model initialization: {str(e)}")
            sys.exit(1)

    args:
      - --input_dim
      - {inputValue: input_dim}
      - --hidden_dim_1
      - {inputValue: hidden_dim_1}
      - --hidden_dim_2
      - {inputValue: hidden_dim_2}
      - --latent_dim
      - {inputValue: latent_dim}
      - --dropout
      - {inputValue: dropout}
      - --weight_init_method
      - {inputValue: weight_init_method}
      - --activation_function
      - {inputValue: activation_function}
      - --use_batch_norm
      - {inputValue: use_batch_norm}
      - --optimizer_type
      - {inputValue: optimizer_type}
      - --learning_rate
      - {inputValue: learning_rate}
      - --weight_decay
      - {inputValue: weight_decay}
      - --loss_type
      - {inputValue: loss_type}
      - --seed
      - {inputValue: seed}
      - --model_output
      - {outputPath: model_output}
      - --model_config
      - {outputPath: model_config}
