name: Train Denoising Autoencoder Model v3.4
inputs:
  - {name: model_input, type: Data}
  - {name: train_dataset, type: Data}
  - {name: val_dataset, type: Data}
  - {name: epochs, type: Integer, optional: true, default: "100"}
  - {name: batch_size, type: Integer, optional: true, default: "32"}
  - {name: num_workers, type: Integer, optional: true, default: "0"}
  - {name: noise_type, type: String, optional: true, default: "gaussian"}
  - {name: noise_factor, type: Float, optional: true, default: "0.3"}
  - {name: early_stopping_patience, type: Integer, optional: true, default: "10"}
  - {name: scheduler_type, type: String, optional: true, default: "reduce_on_plateau"}
  - {name: scheduler_factor, type: Float, optional: true, default: "0.5"}
  - {name: scheduler_patience, type: Integer, optional: true, default: "5"}
  - {name: scheduler_min_lr, type: Float, optional: true, default: "0.000001"}
  - {name: seed, type: Integer, optional: true, default: "42"}

outputs:
  - {name: trained_model, type: Model}
  - {name: training_history, type: Data}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import logging
        import random
        import numpy as np
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import DataLoader, TensorDataset
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt

        logging.basicConfig(stream=sys.stdout, level=logging.INFO)
        logger = logging.getLogger("model_trainer")

        parser = argparse.ArgumentParser()
        parser.add_argument('--model_input', required=True)
        parser.add_argument('--train_dataset', required=True)
        parser.add_argument('--val_dataset', required=True)
        parser.add_argument('--epochs', type=int, default=100)
        parser.add_argument('--batch_size', type=int, default=32)
        parser.add_argument('--num_workers', type=int, default=0)
        parser.add_argument('--noise_type', type=str, default='gaussian')
        parser.add_argument('--noise_factor', type=float, default=0.3)
        parser.add_argument('--early_stopping_patience', type=int, default=10)
        parser.add_argument('--scheduler_type', type=str, default='reduce_on_plateau')
        parser.add_argument('--scheduler_factor', type=float, default=0.5)
        parser.add_argument('--scheduler_patience', type=int, default=5)
        parser.add_argument('--scheduler_min_lr', type=float, default=1e-6)
        parser.add_argument('--seed', type=int, default=42)
        parser.add_argument('--trained_model', required=True)
        parser.add_argument('--training_history', required=True)
        args = parser.parse_args()

        def set_seed(seed):
            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
            logger.info(f"✓ Random seed set to: {seed}")

        def get_activation(activation_name):
            activations = {
                'relu': nn.ReLU(), 'leaky_relu': nn.LeakyReLU(0.2),
                'elu': nn.ELU(), 'selu': nn.SELU(), 'gelu': nn.GELU(),
                'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid(),
                'swish': nn.SiLU(), 'mish': nn.Mish(), 'prelu': nn.PReLU(),
            }
            return activations.get(activation_name.lower(), nn.ReLU())

        class Encoder(nn.Module):
            def __init__(self, input_dim, encoder_dims, latent_dim,
                         dropout, activation, use_batch_norm):
                super(Encoder, self).__init__()
                layers = []
                prev_dim = input_dim
                for idx, units in enumerate(encoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    layers.append(nn.Dropout(dropout))
                    logger.info(f"    Encoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                layers.append(nn.Linear(prev_dim, latent_dim))
                logger.info(f"    Encoder Latent : {prev_dim} -> {latent_dim}")
                self.encoder = nn.Sequential(*layers)

            def forward(self, x):
                return self.encoder(x)

        class Decoder(nn.Module):
            def __init__(self, latent_dim, decoder_dims, input_dim,
                         dropout, activation, use_batch_norm):
                super(Decoder, self).__init__()
                layers = []
                prev_dim = latent_dim
                for idx, units in enumerate(decoder_dims):
                    layers.append(nn.Linear(prev_dim, units))
                    if use_batch_norm:
                        layers.append(nn.BatchNorm1d(units))
                    layers.append(get_activation(activation))
                    layers.append(nn.Dropout(dropout))
                    logger.info(f"    Decoder Layer {idx + 1}: {prev_dim} -> {units}")
                    prev_dim = units
                layers.append(nn.Linear(prev_dim, input_dim))
                logger.info(f"    Decoder Output : {prev_dim} -> {input_dim}")
                self.decoder = nn.Sequential(*layers)

            def forward(self, z):
                return self.decoder(z)

        class DenoisingAutoencoder(nn.Module):
            def __init__(self, input_dim, encoder_dims, decoder_dims, latent_dim,
                         dropout, activation, use_batch_norm):
                super(DenoisingAutoencoder, self).__init__()
                self.input_dim  = input_dim
                self.latent_dim = latent_dim
                self.encoder = Encoder(input_dim, encoder_dims, latent_dim,
                                       dropout, activation, use_batch_norm)
                self.decoder = Decoder(latent_dim, decoder_dims, input_dim,
                                       dropout, activation, use_batch_norm)

            def forward(self, x):
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded, encoded

            def encode(self, x): return self.encoder(x)
            def decode(self, z): return self.decoder(z)

        def add_noise_to_latent(latent_vectors, noise_type='gaussian', noise_factor=0.3):
            if noise_type == 'gaussian':
                return latent_vectors + torch.randn_like(latent_vectors) * noise_factor
            elif noise_type == 'salt_pepper':
                noisy = latent_vectors.clone()
                mask = torch.rand_like(latent_vectors) < noise_factor
                random_vals = torch.rand(mask.sum(), device=latent_vectors.device)
                random_vals = random_vals * (latent_vectors.max() - latent_vectors.min()) + latent_vectors.min()
                noisy[mask] = random_vals
                return noisy
            elif noise_type == 'dropout':
                return latent_vectors * (torch.rand_like(latent_vectors) > noise_factor)
            elif noise_type == 'uniform':
                return latent_vectors + (torch.rand_like(latent_vectors) - 0.5) * 2 * noise_factor
            else:
                raise ValueError(f"Unknown noise type: {noise_type}")

        def setup_scheduler(optimizer, scheduler_type='reduce_on_plateau', **kwargs):
            if scheduler_type is None or scheduler_type.lower() == 'none':
                return None
            if scheduler_type.lower() == 'reduce_on_plateau':
                return optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min',
                    factor=kwargs.get('factor', 0.5),
                    patience=kwargs.get('patience', 5),
                    min_lr=kwargs.get('min_lr', 1e-6))
            elif scheduler_type.lower() == 'cosine':
                return optim.lr_scheduler.CosineAnnealingLR(
                    optimizer, T_max=kwargs.get('T_max', 50), eta_min=kwargs.get('eta_min', 1e-6))
            elif scheduler_type.lower() == 'step':
                return optim.lr_scheduler.StepLR(
                    optimizer, step_size=kwargs.get('step_size', 30), gamma=kwargs.get('gamma', 0.1))
            elif scheduler_type.lower() == 'exponential':
                return optim.lr_scheduler.ExponentialLR(optimizer, gamma=kwargs.get('gamma', 0.95))
            else:
                raise ValueError(f"Unknown scheduler: {scheduler_type}")

        class DAETrainer:
            def __init__(self, model, device):
                self.model = model.to(device)
                self.device = device
                self.train_losses = []
                self.val_losses   = []

            def train_epoch(self, dataloader, criterion, optimizer, noise_type, noise_factor):
                self.model.train()
                epoch_loss = 0.0
                for (clean_latents,) in dataloader:
                    clean_latents = clean_latents.to(self.device)
                    noisy_latents = add_noise_to_latent(clean_latents, noise_type, noise_factor)
                    reconstructed, _ = self.model(noisy_latents)
                    loss = criterion(reconstructed, clean_latents)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    epoch_loss += loss.item()
                return epoch_loss / len(dataloader)

            def validate_epoch(self, dataloader, criterion, noise_type, noise_factor):
                self.model.eval()
                epoch_loss = 0.0
                with torch.no_grad():
                    for (clean_latents,) in dataloader:
                        clean_latents = clean_latents.to(self.device)
                        noisy_latents = add_noise_to_latent(clean_latents, noise_type, noise_factor)
                        reconstructed, _ = self.model(noisy_latents)
                        epoch_loss += criterion(reconstructed, clean_latents).item()
                return epoch_loss / len(dataloader)

            def fit(self, train_loader, val_loader, criterion, optimizer,
                    model_config, optimizer_config, loss_type, 
                    scheduler=None, epochs=100,
                    noise_type='gaussian', noise_factor=0.3,
                    early_stopping_patience=10, save_path='best_dae_model.pth'):

                best_val_loss    = float('inf')
                patience_counter = 0

                logger.info(f"Training on device: {self.device}")
                logger.info(f"Noise type: {noise_type}, Noise factor: {noise_factor}")
                logger.info("-" * 60)

                for epoch in range(epochs):
                    train_loss = self.train_epoch(train_loader, criterion, optimizer, noise_type, noise_factor)
                    val_loss   = self.validate_epoch(val_loader, criterion, noise_type, noise_factor)

                    self.train_losses.append(train_loss)
                    self.val_losses.append(val_loss)

                    if scheduler is not None:
                        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                            scheduler.step(val_loss)
                        else:
                            scheduler.step()

                    current_lr = optimizer.param_groups[0]['lr']
                    logger.info(f"Epoch [{epoch+1}/{epochs}] | "
                                f"Train Loss: {train_loss:.6f} | "
                                f"Val Loss: {val_loss:.6f} | "
                                f"LR: {current_lr:.6f}")

                    if val_loss < best_val_loss:
                        best_val_loss    = val_loss
                        patience_counter = 0
                        torch.save({
                            'epoch':                epoch,
                            'model_state_dict':     self.model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'train_loss':           train_loss,
                            'val_loss':             val_loss,
                            'model_config':         model_config,      
                            'optimizer_config':     optimizer_config,  
                            'loss_type':            loss_type,         
                        }, save_path)
                        logger.info(f"✓ Best model saved (Val Loss: {val_loss:.6f})")
                    else:
                        patience_counter += 1
                        if patience_counter >= early_stopping_patience:
                            logger.info(f"Early stopping triggered after {epoch+1} epochs")
                            break

                logger.info("=" * 60)
                logger.info(f"Training completed! Best Val Loss: {best_val_loss:.6f}")
                logger.info("=" * 60)

                loaded = torch.load(save_path)
                self.model.load_state_dict(loaded['model_state_dict'])
                return self.train_losses, self.val_losses

        def get_loss_function(loss_type):
            return {'mse': nn.MSELoss(), 'l1': nn.L1Loss(),
                    'smooth_l1': nn.SmoothL1Loss(), 'huber': nn.HuberLoss(delta=1.0)}.get(
                    loss_type.lower(), nn.MSELoss())

        def get_optimizer(model, optimizer_type, lr, weight_decay):
            if optimizer_type.lower() == 'adam':
                return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
            elif optimizer_type.lower() == 'adamw':
                return optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
            elif optimizer_type.lower() == 'sgd':
                return optim.SGD(model.parameters(), lr=lr, momentum=0.9,
                                 weight_decay=weight_decay, nesterov=True)
            elif optimizer_type.lower() == 'rmsprop':
                return optim.RMSprop(model.parameters(), lr=lr,
                                     weight_decay=weight_decay, momentum=0.9)
            else:
                return optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)

        try:
            set_seed(args.seed)

            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            if torch.cuda.is_available():
                logger.info(f"✓ GPU detected: {torch.cuda.get_device_name(0)}")
                logger.info(f"  GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
            else:
                logger.info("⚠ No GPU detected, using CPU")
            logger.info("=" * 60)
            logger.info("Loading Model Checkpoint")
            logger.info("=" * 60)

            model_checkpoint_path = os.path.join(args.model_input, 'initialized_model.pt')
            if not os.path.exists(model_checkpoint_path):
                raise FileNotFoundError(f"Model checkpoint not found at: {model_checkpoint_path}")

            checkpoint       = torch.load(model_checkpoint_path, map_location=device)
            model_config     = checkpoint['model_config']
            optimizer_config = checkpoint['optimizer_config']
            loss_type        = checkpoint['loss_type']

            logger.info(f"Model config     : {model_config}")
            logger.info(f"Optimizer config : {optimizer_config}")
            logger.info(f"Loss type        : {loss_type}")
            if 'noise' not in model_config:
                model_config['noise'] = {}
            model_config['noise']['noise_type']   = args.noise_type
            model_config['noise']['noise_factor'] = args.noise_factor
            logger.info(f"✓ Updated model_config with noise params:")
            logger.info(f"  noise_type   : {args.noise_type}")
            logger.info(f"  noise_factor : {args.noise_factor}")

            logger.info("=" * 60)
            logger.info("Reconstructing Model")
            logger.info("=" * 60)

            model = DenoisingAutoencoder(
                input_dim      = model_config['input_dim'],
                encoder_dims   = model_config['encoder_dims'],
                decoder_dims   = model_config['decoder_dims'],
                latent_dim     = model_config['latent_dim'],
                dropout        = model_config['dropout'],
                activation     = model_config.get('activation_function', 'relu'),
                use_batch_norm = model_config.get('use_batch_norm', True)
            )
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(device)

            logger.info(f"✓ Model reconstructed to {device}")
            logger.info(f"  Encoder layers   : {model_config['encoder_dims']}")
            logger.info(f"  Decoder layers   : {model_config['decoder_dims']}")
            logger.info(f"  Total parameters : {sum(p.numel() for p in model.parameters()):,}")

            # Load Datasets
            logger.info("=" * 60)
            logger.info("Loading Datasets")
            logger.info("=" * 60)

            train_data    = torch.load(os.path.join(args.train_dataset, 'train_dataset.pt'), map_location='cpu')
            val_data      = torch.load(os.path.join(args.val_dataset,   'val_dataset.pt'),   map_location='cpu')
            train_dataset = TensorDataset(train_data)
            val_dataset   = TensorDataset(val_data)
            logger.info(f"✓ Train dataset: {train_data.shape}")
            logger.info(f"✓ Val dataset  : {val_data.shape}")

            train_loader = DataLoader(train_dataset, batch_size=args.batch_size,
                                      shuffle=True,  num_workers=args.num_workers,
                                      pin_memory=True if device == 'cuda' else False)
            val_loader   = DataLoader(val_dataset,   batch_size=args.batch_size,
                                      shuffle=False, num_workers=args.num_workers,
                                      pin_memory=True if device == 'cuda' else False)
            logger.info(f"✓ Train batches : {len(train_loader)}")
            logger.info(f"✓ Val batches   : {len(val_loader)}")

            
            # Setup Training Components
            criterion = get_loss_function(loss_type)
            optimizer = get_optimizer(model, optimizer_config['type'],
                                      optimizer_config['lr'], optimizer_config['weight_decay'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            scheduler = setup_scheduler(optimizer, scheduler_type=args.scheduler_type,
                                        factor=args.scheduler_factor, patience=args.scheduler_patience,
                                        min_lr=args.scheduler_min_lr, T_max=args.epochs)

            # Train
            logger.info("=" * 60)
            logger.info("Starting Training")
            logger.info("=" * 60)
            logger.info(f"Epochs                  : {args.epochs}")
            logger.info(f"Batch size              : {args.batch_size}")
            logger.info(f"Noise type              : {args.noise_type}")
            logger.info(f"Noise factor            : {args.noise_factor}")
            logger.info(f"Early stopping patience : {args.early_stopping_patience}")

            os.makedirs(args.trained_model, exist_ok=True)
            best_model_path = os.path.join(args.trained_model, 'best_trained_model.pth')

            trainer = DAETrainer(model, device=device)
            train_losses, val_losses = trainer.fit(
                train_loader=train_loader,
                val_loader=val_loader,
                criterion=criterion,
                optimizer=optimizer,
                model_config=model_config,
                optimizer_config=optimizer_config,
                loss_type=loss_type,
                scheduler=scheduler,
                epochs=args.epochs,
                noise_type=args.noise_type,
                noise_factor=args.noise_factor,
                early_stopping_patience=args.early_stopping_patience,
                save_path=best_model_path
            )

            # Save Training History
            os.makedirs(args.training_history, exist_ok=True)
            history = {
                'train_losses':    train_losses,
                'val_losses':      val_losses,
                'epochs_trained':  len(train_losses),
                'best_val_loss':   min(val_losses),
                'final_train_loss': train_losses[-1],
                'final_val_loss':   val_losses[-1],
                'training_config': {
                    'epochs':                   args.epochs,
                    'batch_size':               args.batch_size,
                    'noise_type':               args.noise_type,
                    'noise_factor':             args.noise_factor,
                    'scheduler_type':           args.scheduler_type,
                    'early_stopping_patience':  args.early_stopping_patience
                }
            }
            history_path = os.path.join(args.training_history, 'training_history.json')
            with open(history_path, 'w') as f:
                json.dump(history, f, indent=2)

            with open(args.training_history + ".meta.json", "w") as f:
                json.dump({"history_path": history_path,
                           "epochs_trained": len(train_losses),
                           "best_val_loss": min(val_losses)}, f, indent=2)

            with open(args.trained_model + ".meta.json", "w") as f:
                json.dump({"model_path": best_model_path,
                           "best_val_loss": min(val_losses),
                           "epochs_trained": len(train_losses)}, f, indent=2)

            logger.info("=" * 60)
            logger.info("✓ Training Complete!")
            logger.info("=" * 60)
            logger.info(f"Best model       : {best_model_path}")
            logger.info(f"Training history : {history_path}")
            logger.info(f"Best val loss    : {min(val_losses):.6f}")
            logger.info(f"Epochs trained   : {len(train_losses)}")

        except Exception as e:
            logger.exception(f"Fatal error during training: {str(e)}")
            sys.exit(1)

    args:
      - --model_input
      - {inputPath: model_input}
      - --train_dataset
      - {inputPath: train_dataset}
      - --val_dataset
      - {inputPath: val_dataset}
      - --epochs
      - {inputValue: epochs}
      - --batch_size
      - {inputValue: batch_size}
      - --num_workers
      - {inputValue: num_workers}
      - --noise_type
      - {inputValue: noise_type}
      - --noise_factor
      - {inputValue: noise_factor}
      - --early_stopping_patience
      - {inputValue: early_stopping_patience}
      - --scheduler_type
      - {inputValue: scheduler_type}
      - --scheduler_factor
      - {inputValue: scheduler_factor}
      - --scheduler_patience
      - {inputValue: scheduler_patience}
      - --scheduler_min_lr
      - {inputValue: scheduler_min_lr}
      - --seed
      - {inputValue: seed}
      - --trained_model
      - {outputPath: trained_model}
      - --training_history
      - {outputPath: training_history}
